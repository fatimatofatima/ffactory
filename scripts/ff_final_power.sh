#!/usr/bin/env bash
set -Eeuo pipefail

# ๐ฏ FFactory Ultimate Power Script - 100% Operational Force
log(){ echo "๐ข $(date '+%Y-%m-%d %H:%M:%S') - $*"; }
warn(){ echo "๐ก $(date '+%Y-%m-%d %H:%M:%S') - $*" >&2; }
die(){ echo "๐ด $(date '+%Y-%m-%d %H:%M:%S') - $*" >&2; exit 1; }

FF="/opt/ffactory"
COMPOSE_DIR="$FF/stack"
APPS="$FF/apps"
SCRIPTS="$FF/scripts"
DATA_DIR="$FF/data"

# ๐ง ุญู ุงููุดุงูู ุงูุฌููุฑูุฉ ุงููุฐููุฑุฉ ูู ุงูุชูุฑูุฑ
solve_core_issues() {
    log "1. ุญู ุงููุดุงูู ุงูุฌููุฑูุฉ ุงููุฐููุฑุฉ ูู ุงูุชูุฑูุฑ..."
    
    # ๐ ุญู ูุดููุฉ Connection Refused ูู Correlation Engine
    log "๐ ุญู ูุดููุฉ Neo4j Connection Refused..."
    sudo tee "$SCRIPTS/wait-for-neo4j.sh" >/dev/null <<'WAIT_NEO4J'
#!/bin/bash
set -e
echo "โณ ุงูุชุธุงุฑ Neo4j Bolt ุนูู neo4j:7687..."
until nc -z neo4j 7687; do
    echo "โฑ๏ธ Neo4j ุบูุฑ ุฌุงูุฒ ุจุนุฏ... ุงูุงูุชุธุงุฑ 5 ุซูุงู"
    sleep 5
done
echo "โ Neo4j ุฌุงูุฒ ููุงุชุตุงู!"
WAIT_NEO4J
    chmod +x "$SCRIPTS/wait-for-neo4j.sh"

    # ๐๏ธ ุญู ูุดููุฉ PostgreSQL ุนูู ุงููููุฐ 5433
    log "๐๏ธ ุญู ูุดููุฉ PostgreSQL Port 5433..."
    sudo tee "$SCRIPTS/wait-for-postgres.sh" >/dev/null <<'WAIT_POSTGRES'
#!/bin/bash
set -e
echo "โณ ุงูุชุธุงุฑ PostgreSQL ุนูู db:5433..."
until pg_isready -h db -p 5433 -U ${POSTGRES_USER}; do
    echo "โฑ๏ธ PostgreSQL ุบูุฑ ุฌุงูุฒ ุจุนุฏ... ุงูุงูุชุธุงุฑ 5 ุซูุงู"
    sleep 5
done
echo "โ PostgreSQL ุฌุงูุฒ ููุงุชุตุงู!"
WAIT_POSTGRES
    chmod +x "$SCRIPTS/wait-for-postgres.sh"
}

# ๐ฏ ุฅูุดุงุก ูููุงุช Docker Compose ุงูููุญุณูููุฉ
create_optimized_compose() {
    log "2. ุฅูุดุงุก ูููุงุช Docker Compose ููุญููููุฉ ุงููุดุงูู..."
    
    # ๐ฆ ููู ุงูุฎุฏูุงุช ุงูุฃุณุงุณูุฉ ูุน Health Checks
    sudo tee "$COMPOSE_DIR/docker-compose.core.yml" >/dev/null <<'CORE_ENHANCED'
version: '3.8'

services:
  # ๐๏ธ PostgreSQL ูุน ุฅุนุฏุงุฏุงุช ูุชูุฏูุฉ
  postgres:
    image: postgres:15-alpine
    container_name: ffactory_db
    environment:
      - POSTGRES_DB=ffactory_forensic
      - POSTGRES_USER=ffadmin
      - POSTGRES_PASSWORD=Aa100200@@
      - PGPORT=5433
    ports:
      - "5433:5433"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ../scripts/wait-for-postgres.sh:/wait-for-postgres.sh
    command: [ "postgres", "-p", "5433", "-c", "listen_addresses=*" ]
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -p 5433 -U ffadmin" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - ffactory_network

  # ๐ฎ Neo4j ูุน ุฅุนุฏุงุฏุงุช ุฃูุงู ูุญุณูุฉ
  neo4j:
    image: neo4j:5.12
    container_name: ffactory_neo4j
    environment:
      - NEO4J_AUTH=neo4j/Forensic123!
      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes
      - NEO4J_dbms_connector_bolt_listen__address=neo4j:7687
      - NEO4J_dbms_connector_http_listen__address=neo4j:7474
      - NEO4J_dbms_connector_https_listen__address=neo4j:7473
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - ../scripts/wait-for-neo4j.sh:/wait-for-neo4j.sh
    healthcheck:
      test: [ "CMD", "cypher-shell", "-u", "neo4j", "-p", "Forensic123!", "RETURN 1" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - ffactory_network

  # โ๏ธ MinIO ููุชุฎุฒูู
  minio:
    image: minio/minio
    container_name: ffactory_minio
    environment:
      - MINIO_ROOT_USER=ffminio
      - MINIO_ROOT_PASSWORD=Mini0123@@
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - ffactory_network

volumes:
  postgres_data:
  neo4j_data:
  neo4j_logs:
  minio_data:

networks:
  ffactory_network:
    driver: bridge
CORE_ENHANCED

    # ๐ค ููู ูุญุฑูุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงูุญููููุฉ
    sudo tee "$COMPOSE_DIR/docker-compose.ai.yml" >/dev/null <<'AI_ENHANCED'
version: '3.8'

services:
  # ๐ Correlation Engine ุงูุญูููู (ุจุฏูู Stubs)
  correlation-engine:
    build:
      context: ../apps/correlation-engine
      dockerfile: Dockerfile
    container_name: ffactory_correlation
    environment:
      - DB_HOST=postgres
      - DB_PORT=5433
      - DB_USER=ffadmin
      - DB_PASSWORD=Aa100200@@
      - DB_NAME=ffactory_forensic
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=Forensic123!
    depends_on:
      postgres:
        condition: service_healthy
      neo4j:
        condition: service_healthy
    ports:
      - "8082:8080"
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - ffactory_network

  # ๐ค ASR Engine ุงูุญูููู (ุจุฏูู Stubs)
  asr-engine:
    build:
      context: ../apps/asr-engine
      dockerfile: Dockerfile
    container_name: ffactory_asr
    environment:
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN:-hf_yourtokenhere}
      - MODEL_SIZE=medium
      - LANGUAGE=ar
    ports:
      - "8080:8080"
    volumes:
      - asr_cache:/root/.cache
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - ffactory_network

  # ๐ง Neural Core ุงูุญูููู (ุจุฏูู Stubs)
  neural-core:
    build:
      context: ../apps/neural-core
      dockerfile: Dockerfile
    container_name: ffactory_nlp
    environment:
      - HF_TOKEN=${HUGGINGFACE_TOKEN:-hf_yourtokenhere}
      - MODEL_NAME=CAMeL-Lab/bert-base-arabic-camelbert-msa
    ports:
      - "8000:8000"
    volumes:
      - nlp_cache:/root/.cache
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - ffactory_network

volumes:
  asr_cache:
  nlp_cache:

networks:
  ffactory_network:
    external: true
    name: ffactory_ffactory_network
AI_ENHANCED
}

# ๐ง ุงุณุชุจุฏุงู ุงูู Stubs ุจููุฏ ุญูููู
replace_stubs_with_real_code() {
    log "3. ุงุณุชุจุฏุงู ุงูู Stubs ุจููุฏ ุฅูุชุงุฌู ุญูููู..."
    
    # ๐ Correlation Engine ุงูุญูููู
    log "ุจูุงุก Correlation Engine ุงูุญูููู..."
    mkdir -p "$APPS/correlation-engine"
    
    sudo tee "$APPS/correlation-engine/Dockerfile" >/dev/null <<'CORRELATION_DOCKERFILE'
FROM python:3.11-slim

RUN apt-get update && apt-get install -y \
    postgresql-client \
    netcat-openbsd \
    curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# ุณูุฑุจุช ุจุฏุก ุชุดุบูู ุฐูู ููุชุธุฑ ุงูุฎุฏูุงุช
COPY ../scripts/wait-for-neo4j.sh /wait-for-neo4j.sh
COPY ../scripts/wait-for-postgres.sh /wait-for-postgres.sh

CMD ["sh", "-c", "/wait-for-postgres.sh && /wait-for-neo4j.sh && python main.py"]
CORRELATION_DOCKERFILE

    sudo tee "$APPS/correlation-engine/requirements.txt" >/dev/null <<'CORRELATION_REQUIREMENTS'
fastapi>=0.104.0
uvicorn>=0.24.0
asyncpg>=0.28.0
neo4j>=5.14.0
pydantic>=2.0.0
python-multipart>=0.0.6
CORRELATION_REQUIREMENTS

    sudo tee "$APPS/correlation-engine/main.py" >/dev/null <<'CORRELATION_REAL_CODE'
from fastapi import FastAPI, HTTPException
import asyncpg
from neo4j import GraphDatabase
import os
import asyncio

app = FastAPI(title="Correlation Engine - Real Production")

class RealCorrelationEngine:
    def __init__(self):
        self.pg_pool = None
        self.neo4j_driver = None
    
    async def init_databases(self):
        """ุชููุฆุฉ ุงุชุตุงูุงุช ููุงุนุฏ ุงูุจูุงูุงุช ุงูุญููููุฉ"""
        try:
            # ุงุชุตุงู PostgreSQL ุงูุญูููู
            self.pg_pool = await asyncpg.create_pool(
                "postgresql://ffadmin:Aa100200@@@postgres:5433/ffactory_forensic"
            )
            
            # ุงุชุตุงู Neo4j ุงูุญูููู
            self.neo4j_driver = GraphDatabase.driver(
                "bolt://neo4j:7687",
                auth=("neo4j", "Forensic123!")
            )
            
            # ุฅูุดุงุก ูููุฏ Neo4j ููุญุตูู ุนูู ุฃุฏุงุก ุฃูุถู
            with self.neo4j_driver.session() as session:
                session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.id IS UNIQUE")
                session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (f:File) REQUIRE f.hash IS UNIQUE")
                session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (e:Event) REQUIRE e.event_id IS UNIQUE")
            
            print("โ ุชู ุชููุฆุฉ ูุญุฑู ุงูุชุฑุงุจุท ุงูุญูููู")
            return True
        except Exception as e:
            print(f"๐ด ูุดู ุชููุฆุฉ ููุงุนุฏ ุงูุจูุงูุงุช: {e}")
            return False

engine = RealCorrelationEngine()

@app.on_event("startup")
async def startup_event():
    """ุญุฏุซ ุจุฏุก ุงูุชุดุบูู - ููุชุธุฑ ุงูุฎุฏูุงุช"""
    print("โณ ุจุฏุก ุชููุฆุฉ Correlation Engine...")
    success = await engine.init_databases()
    if not success:
        print("๐ด ูุดู ุชููุฆุฉ ุงููุญุฑู - ุณูุนูู ูู ูุถุน ูุชุฏููุฑ")

@app.get("/health")
async def health_check():
    """ูุญุต ุตุญุฉ ูุชูุฏู"""
    try:
        # ูุญุต PostgreSQL
        if engine.pg_pool:
            async with engine.pg_pool.acquire() as conn:
                await conn.fetchval("SELECT 1")
        
        # ูุญุต Neo4j
        if engine.neo4j_driver:
            engine.neo4j_driver.verify_connectivity()
        
        return {
            "status": "healthy",
            "service": "correlation-engine",
            "postgres": "connected",
            "neo4j": "connected",
            "version": "2.0.0-real"
        }
    except Exception as e:
        return {
            "status": "degraded",
            "error": str(e),
            "service": "correlation-engine"
        }

@app.post("/etl/run")
async def run_etl():
    """ุชุดุบูู ุนูููุฉ ETL ุญููููุฉ"""
    try:
        # ุงุณุชุฎุฑุงุฌ ุงูุจูุงูุงุช ูู PostgreSQL
        async with engine.pg_pool.acquire() as conn:
            # ุงูุชุฑุงุถ ูุฌูุฏ ุฌุฏุงูู ุญููููุฉ
            persons = await conn.fetch("""
                SELECT id, name, email, created_at 
                FROM persons 
                LIMIT 100
            """)
            
            files = await conn.fetch("""
                SELECT hash, filename, owner_id, size, created_at 
                FROM files 
                LIMIT 100
            """)
        
        # ุชุญููู ุงูุจูุงูุงุช ุฅูู Neo4j
        with engine.neo4j_driver.session() as session:
            # ุชุญููู ุงูุฃุดุฎุงุต
            for person in persons:
                session.run("""
                    MERGE (p:Person {id: $id})
                    SET p.name = $name, 
                        p.email = $email,
                        p.created_at = $created_at
                """, dict(person))
            
            # ุชุญููู ุงููููุงุช
            for file in files:
                session.run("""
                    MERGE (f:File {hash: $hash})
                    SET f.filename = $filename,
                        f.size = $size,
                        f.created_at = $created_at
                """, dict(file))
            
            # ุฅูุดุงุก ุงูุนูุงูุงุช
            for file in files:
                session.run("""
                    MATCH (p:Person {id: $owner_id})
                    MATCH (f:File {hash: $hash})
                    MERGE (p)-[r:OWNS]->(f)
                    SET r.created_at = $created_at
                """, dict(file))
        
        return {
            "status": "success",
            "message": "ุชู ุชูููุฐ ETL ุจูุฌุงุญ",
            "processed": {
                "persons": len(persons),
                "files": len(files)
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ูุดู ETL: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")
CORRELATION_REAL_CODE

    # ๐ค ASR Engine ุงูุญูููู
    log "ุจูุงุก ASR Engine ุงูุญูููู..."
    sudo tee "$APPS/asr-engine/main.py" >/dev/null <<'ASR_REAL_CODE'
from fastapi import FastAPI, HTTPException, UploadFile, File
from pydantic import BaseModel
import uvicorn
import tempfile
import os
import requests

# ุงุณุชูุฑุงุฏ ุงูููุชุจุงุช ุงูุญููููุฉ
try:
    from faster_whisper import WhisperModel
    from pyannote.audio import Pipeline
    import torch
    WHISPER_READY = True
except ImportError:
    WHISPER_READY = False
    print("๐ด ูู ูุชู ุชุญููู ููุชุจุงุช ASR - ุงููุถุน ุงูุชุฌุฑูุจู")

app = FastAPI(title="ASR Engine - Real Production")

class TranscriptionRequest(BaseModel):
    audio_url: str = None
    language: str = "ar"

# ุชุญููู ุงูููุงุฐุฌ ุงูุญููููุฉ
WHISPER_MODEL = None
DIARIZATION_PIPELINE = None

@app.on_event("startup")
async def startup_event():
    global WHISPER_MODEL, DIARIZATION_PIPELINE
    
    if not WHISPER_READY:
        print("๐ด ุงูุนูู ูู ูุถุน ASR ุงูุชุฌุฑูุจู")
        return
    
    try:
        # ุชุญููู ูููุฐุฌ Whisper ุงูุญูููู
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"๐ข ุชุญููู Whisper ุนูู {device}...")
        WHISPER_MODEL = WhisperModel(
            "medium", 
            device=device, 
            compute_type="int8",
            download_root="/root/.cache/whisper"
        )
        
        # ุชุญููู ูููุฐุฌ Diarization ุฅุฐุง ูุงู ููุงู ุชููู
        hf_token = os.getenv("HUGGINGFACE_TOKEN")
        if hf_token and hf_token != "hf_yourtokenhere":
            print("๐ข ุชุญููู PyAnnote Diarization...")
            DIARIZATION_PIPELINE = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token
            )
            if torch.cuda.is_available():
                DIARIZATION_PIPELINE.to(torch.device("cuda"))
        
        print("โ ุชู ุชุญููู ููุงุฐุฌ ASR ุงูุญููููุฉ")
    except Exception as e:
        print(f"๐ด ูุดู ุชุญููู ุงูููุงุฐุฌ: {e}")

@app.get("/health")
async def health_check():
    status = "healthy" if WHISPER_READY else "degraded"
    return {
        "status": status,
        "service": "asr-engine",
        "whisper_ready": WHISPER_MODEL is not None,
        "diarization_ready": DIARIZATION_PIPELINE is not None,
        "version": "2.0.0-real"
    }

@app.post("/transcribe")
async def transcribe_audio(request: TranscriptionRequest):
    if not WHISPER_MODEL:
        raise HTTPException(status_code=503, detail="ุฎุฏูุฉ ASR ุบูุฑ ุฌุงูุฒุฉ ุจุนุฏ")
    
    try:
        # ุชุญููู ุงูููู ูู URL
        if request.audio_url:
            response = requests.get(request.audio_url, timeout=30)
            response.raise_for_status()
            
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                tmp_file.write(response.content)
                audio_path = tmp_file.name
        else:
            raise HTTPException(status_code=400, detail="ูุฌุจ ุชูููุฑ audio_url")
        
        # ุงูุชูุฑูุบ ุงูุตูุชู ุงูุญูููู
        segments, info = WHISPER_MODEL.transcribe(
            audio_path,
            language=request.language,
            beam_size=5,
            best_of=5
        )
        
        transcription = " ".join(segment.text for segment in segments)
        
        # ุชูููุฒ ุงููุชุญุฏุซูู ุฅุฐุง ูุงู ุงููููุฐุฌ ุฌุงูุฒ
        diarization_result = None
        if DIARIZATION_PIPELINE:
            try:
                diarization = DIARIZATION_PIPELINE(audio_path)
                diarization_result = [
                    {
                        "speaker": turn.speaker,
                        "start": round(turn.start, 2),
                        "end": round(turn.end, 2)
                    }
                    for turn in diarization.itertracks(yield_label=True)
                ]
            except Exception as e:
                print(f"๐ด ูุดู ุชูููุฒ ุงููุชุญุฏุซูู: {e}")
        
        # ุชูุธูู ุงูููู ุงููุคูุช
        os.unlink(audio_path)
        
        return {
            "status": "success",
            "transcription": transcription,
            "language": info.language,
            "language_probability": round(info.language_probability, 3),
            "diarization": diarization_result,
            "model": "faster-whisper-medium"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ูุดู ุงูุชูุฑูุบ: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")
ASR_REAL_CODE

    # ๐ง Neural Core ุงูุญูููู
    log "ุจูุงุก Neural Core ุงูุญูููู..."
    sudo tee "$APPS/neural-core/main.py" >/dev/null <<'NLP_REAL_CODE'
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import os

# ุงุณุชูุฑุงุฏ ููุชุจุงุช NLP ุงูุญููููุฉ
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification
    import torch
    NLP_READY = True
except ImportError:
    NLP_READY = False
    print("๐ด ูู ูุชู ุชุญููู ููุชุจุงุช NLP - ุงููุถุน ุงูุชุฌุฑูุจู")

app = FastAPI(title="Neural Core - Real Production")

class AnalysisRequest(BaseModel):
    text: str
    language: str = "ar"

# ุงูููุงุฐุฌ ุงูุญููููุฉ
NER_PIPELINE = None
SENTIMENT_PIPELINE = None

@app.on_event("startup")
async def startup_event():
    global NER_PIPELINE, SENTIMENT_PIPELINE
    
    if not NLP_READY:
        print("๐ด ุงูุนูู ูู ูุถุน NLP ุงูุชุฌุฑูุจู")
        return
    
    try:
        device = 0 if torch.cuda.is_available() else -1
        
        # ุชุญููู ูููุฐุฌ NER ููุนุฑุจูุฉ
        print("๐ข ุชุญููู ูููุฐุฌ NER ุงูุนุฑุจู...")
        NER_PIPELINE = pipeline(
            "ner",
            model="CAMeL-Lab/bert-base-arabic-camelbert-msa-ner",
            device=device,
            aggregation_strategy="simple"
        )
        
        # ุชุญููู ูููุฐุฌ ุชุญููู ุงููุดุงุนุฑ
        print("๐ข ุชุญููู ูููุฐุฌ ุชุญููู ุงููุดุงุนุฑ...")
        SENTIMENT_PIPELINE = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-xlm-sentiment-multilingual",
            device=device
        )
        
        print("โ ุชู ุชุญููู ููุงุฐุฌ NLP ุงูุญููููุฉ")
    except Exception as e:
        print(f"๐ด ูุดู ุชุญููู ุงูููุงุฐุฌ: {e}")

@app.get("/health")
async def health_check():
    status = "healthy" if NLP_READY else "degraded"
    return {
        "status": status,
        "service": "neural-core",
        "ner_ready": NER_PIPELINE is not None,
        "sentiment_ready": SENTIMENT_PIPELINE is not None,
        "version": "2.0.0-real"
    }

@app.post("/analyze/ner")
async def analyze_ner(request: AnalysisRequest):
    if not NER_PIPELINE:
        raise HTTPException(status_code=503, detail="ุฎุฏูุฉ NER ุบูุฑ ุฌุงูุฒุฉ ุจุนุฏ")
    
    try:
        # ุงุณุชุฎุฑุงุฌ ุงูููุงูุงุช ุงููุณูุงุฉ ุงูุญููููุฉ
        entities = NER_PIPELINE(request.text)
        
        return {
            "status": "success",
            "text": request.text,
            "entities": entities,
            "model": "CAMeL-Lab/bert-base-arabic-camelbert-msa-ner"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ูุดู ุชุญููู NER: {str(e)}")

@app.post("/analyze/sentiment")
async def analyze_sentiment(request: AnalysisRequest):
    if not SENTIMENT_PIPELINE:
        raise HTTPException(status_code=503, detail="ุฎุฏูุฉ ุชุญููู ุงููุดุงุนุฑ ุบูุฑ ุฌุงูุฒุฉ ุจุนุฏ")
    
    try:
        # ุชุญููู ุงููุดุงุนุฑ ุงูุญูููู
        sentiment = SENTIMENT_PIPELINE(request.text)
        
        return {
            "status": "success",
            "text": request.text,
            "sentiment": sentiment[0] if sentiment else {},
            "model": "cardiffnlp/twitter-xlm-sentiment-multilingual"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ูุดู ุชุญููู ุงููุดุงุนุฑ: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
NLP_REAL_CODE
}

# ๐ ุชุดุบูู ุงููุธุงู ุงููุญุณูู
deploy_enhanced_system() {
    log "4. ูุดุฑ ุงููุธุงู ุงููุญุณูู..."
    
    cd "$FF"
    
    # ุฅููุงู ุฃู ุฎุฏูุงุช ุณุงุจูุฉ
    log "ุฅููุงู ุงูุฎุฏูุงุช ุงูุณุงุจูุฉ..."
    docker-compose -f stack/docker-compose.core.yml down 2>/dev/null || true
    docker-compose -f stack/docker-compose.ai.yml down 2>/dev/null || true
    
    # ุจูุงุก ูุชุดุบูู ุงูุฎุฏูุงุช ุงูุฃุณุงุณูุฉ
    log "ุจูุงุก ุงูุฎุฏูุงุช ุงูุฃุณุงุณูุฉ..."
    docker-compose -f stack/docker-compose.core.yml up -d --build
    
    # ุงูุชุธุงุฑ ุงูุฎุฏูุงุช ุงูุฃุณุงุณูุฉ
    log "ุงูุชุธุงุฑ ุชููุฆุฉ ููุงุนุฏ ุงูุจูุงูุงุช..."
    sleep 30
    
    # ูุญุต ุตุญุฉ ุงูุฎุฏูุงุช ุงูุฃุณุงุณูุฉ
    log "ูุญุต ุตุญุฉ ุงูุฎุฏูุงุช ุงูุฃุณุงุณูุฉ..."
    if docker-compose -f stack/docker-compose.core.yml ps | grep -q "Up"; then
        log "โ ุงูุฎุฏูุงุช ุงูุฃุณุงุณูุฉ ุชุนูู ุจูุฌุงุญ"
    else
        die "๐ด ูุดู ุชุดุบูู ุงูุฎุฏูุงุช ุงูุฃุณุงุณูุฉ"
    fi
    
    # ุจูุงุก ูุชุดุบูู ูุญุฑูุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู
    log "ุจูุงุก ูุญุฑูุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงูุญููููุฉ..."
    docker-compose -f stack/docker-compose.ai.yml up -d --build
    
    # ุงูุชุธุงุฑ ูุญุฑูุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู
    log "ุงูุชุธุงุฑ ุชุญููู ููุงุฐุฌ ุงูุฐูุงุก ุงูุงุตุทูุงุนู..."
    sleep 60
    
    # ูุญุต ุตุญุฉ ุงููุธุงู ุงููุงูู
    log "ุฅุฌุฑุงุก ูุญุต ุตุญุฉ ููุงุฆู..."
    check_system_health
}

# ๐ฅ ูุญุต ุตุญุฉ ุงููุธุงู ุงููุชูุฏู
check_system_health() {
    log "5. ูุญุต ุตุญุฉ ุงููุธุงู ุงููุชูุฏู..."
    
    echo ""
    echo "๐ ูุญุต ุตุญุฉ ุงูุฎุฏูุงุช:"
    echo "===================="
    
    # ูุญุต ูู ุฎุฏูุฉ
    services=(
        "postgres:5433"
        "neo4j:7687" 
        "minio:9000"
        "asr-engine:8080"
        "neural-core:8000"
        "correlation-engine:8082"
    )
    
    all_healthy=true
    
    for service in "${services[@]}"; do
        name="${service%:*}"
        port="${service#*:}"
        
        if docker exec "ffactory_$name" curl -f -s "http://localhost:$port/health" >/dev/null 2>&1; then
            echo "โ $name: ุตุญู"
        else
            echo "๐ด $name: ุบูุฑ ุตุญู"
            all_healthy=false
        fi
    done
    
    echo ""
    if $all_healthy; then
        echo "๐ ุฌููุน ุงูุฎุฏูุงุช ุชุนูู ุจุตุญุฉ ููุชุงุฒุฉ!"
        echo ""
        echo "๐ ุฑูุงุจุท ุงููุธุงู:"
        echo "  ๐ Neo4j Browser: http://localhost:7474 (neo4j/Forensic123!)"
        echo "  โ๏ธ MinIO Console: http://localhost:9001 (ffminio/Mini0123@@)"
        echo "  ๐ค ASR Engine: http://localhost:8080"
        echo "  ๐ง NLP Engine: http://localhost:8000" 
        echo "  ๐ Correlation Engine: http://localhost:8082"
        echo ""
        echo "๐ ุงููุธุงู ุฌุงูุฒ ููุงุณุชุฎุฏุงู ุงูุชุดุบููู!"
    else
        echo "โ๏ธ ุจุนุถ ุงูุฎุฏูุงุช ุชุญุชุงุฌ ุฅูู ุงูุชูุงู"
        echo "๐ง ุชุดุบูู 'docker logs <container_name>' ููุชุญูู ูู ุงูุณุฌูุงุช"
    fi
}

# ๐ฏ ุงูุชูููุฐ ุงูุฑุฆูุณู
main() {
    echo "๐ ุจุฏุก ุชูุนูู ุงููุธุงู ุงูุฌูุงุฆู - 100% ููุฉ ุญููููุฉ"
    echo "============================================="
    echo "๐ฏ ุงููุฏู: ุชุญููู ุงููุธุงู ูู ูููู ุชุฌุฑูุจู ุฅูู ูุธุงู ุฅูุชุงุฌู"
    echo ""
    
    solve_core_issues
    create_optimized_compose
    replace_stubs_with_real_code
    deploy_enhanced_system
    
    echo ""
    echo "โ ุชู ุงูุงูุชูุงุก ูู ุชูุนูู ุงููุธุงู ุจูุฌุงุญ!"
    echo "๐ก ุชุฐูุฑ: ูู ุจุชุนููู HUGGINGFACE_TOKEN ุงูุญูููู ูู ููู .env ูุชูุนูู ูุงูู ุงูููุฒุงุช"
}

# ุงูุชุดุบูู
main "$@"
